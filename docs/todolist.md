
## DONEðŸ”¨

- [x] backend
    - [x] numpy
- [x] autograd
    - [x] function
        - [x] forward
        - [x] backward
    - [x] engine
        - [x] backward propagation
    - [x] variable
        - [x] +, -, *, /, **, log, @
        - [x] slice
        - [x] broadcasting
        - [x] reshape, transpose
        - [x] sum, mean, var
        - [x] creation
            - [x] from_numpy
            - [x] rand, randn
            - [x] eye, ones, zeros, *_like
        - [x] inspace ops
            - [x] sub_, add_, mul_, zeros_
- [x] nn
    - [x] parameter
    - [x] modules
        - [x] linear
        - [x] bn2d
            - [x] running_mean/var
            - [x] training/eval
        - [x] gn2d
        - [x] conv2d
            - [x] group
            - [x] dilation
        - [x] pooling2d
            - [x] avg
            - [x] max
        - [x] activation
            - [x] relu
            - [x] tanh
            - [x] sigmoid
        - [x] padding
            - [x] pad2d, 
            - [x] zero, constant
        - [x] softmax
        - [x] dropout
        - [x] attention
            - [x] multi-head
            - [x] self-attention
    - [x] loss
        - [x] mse
        - [x] cross entropy
- [x] optim
    - [x] optimizer
        - [x] sgd
            - [x] momentum
            - [x] nesterov
    - [x] lr_scheduler
        - [x] miles stones
- [x] data
    - [x] dataset
    - [x] dataloader
        - [x] thread pool
- [x] serializable
    - [x] save
    - [x] load

